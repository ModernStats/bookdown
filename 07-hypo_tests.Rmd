# Hypothesis Testing {#hypo}  
    
```{r setup_hypo, include=FALSE}
chap <- 7
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**
knitr::opts_chunk$set(tidy = FALSE, fig.align = "center", out.width='\\textwidth')
```

<!-- 
+ Hypothesis testing
    + Distance of flights from NYC to Boston vs flights from NYC to SF.
-->  
    

<!--
+ Idea of histogram of sample being a visual approximation to population distribution.
    + Shiny
+ Standard errors and sampling distribution via randomization
    + Developing traditional inference from randomization
    + two-sample permutation test -> null distribution  
    + Showing what happens when assumptions/conditions arenâ€™t met
    + Then show normal/t-test and show how it fits on top of sampling distribution
    + Shiny Google Forms
-->

We saw some of the main concepts of hypothesis testing introduced in Chapter \@ref(infer-basics).  We will expand further on these ideas here and also provide a framework for understanding hypothesis tests in general.  Instead of presenting you with lots of different formulas and scenarios, we hope to build a way to think about all hypothesis tests.  You can then adapt to different scenarios as needed down the road when you encounter different statistical situations.

In a hypothesis test, we will use data from a sample to help us decide between two competing _hypotheses_ about a population.  We make these hypotheses more concrete by specifying them in terms of at least one _population parameter_ of interest.  We refer to the competing claims about the population as the **null hypothesis**, denoted by $H_0$, and the **alternative (or research) hypothesis**, denoted by $H_a$.  The roles of these two hypotheses are NOT interchangeable.  

- The claim for which we seek significant evidence is assigned to the alternative hypothesis.  The alternative is usually what the experimenter or researcher wants to establish or find evidence for.
- Usually, the null hypothesis is a claim that there really is "no effect" or "no difference."  In many cases, the null hypothesis represents the status quo or that nothing interesting is happening.  
- We assess the strength of evidence by assuming the null hypothesis is true and determining how unlikely it would be to see sample results as extreme (or more extreme) as those in the original sample.

Hypothesis testing brings about many weird and incorrect notions in the scientific community and society at large.  One reason for this is that statistics has traditionally been thought of as this magic box of algorithms and procedures to get to results and this has been readily apparent if you do a Google search of "flowchart statistics hypothesis tests".  There are so many different complex ways to determine which test is appropriate.  

You'll see that we don't need to rely on these complicated series of assumptions and procedures to conduct a hypothesis test any longer.  These methods were introduced in a time when computers weren't powerful.  Your cellphone (in 2016) has more power than the computers that sent NASA astronauts to the moon after all.  We'll see that ALL hypothesis tests can be broken down into the following framework given by Allen Downey [here](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html):

```{r htdowney, echo=FALSE, fig.cap="Hypothesis Testing Framework"}
knitr::include_graphics("images/ht.png")
```

Before we hop into this framework, we will provide another way to think about hypothesis testing that may be useful.

## Criminal trial analogy {#trial}

We can think of hypothesis testing in the same context as a criminal trial in the United States.  A criminal trial in the United States is a familiar situation in which a choice between two contradictory claims must be made. 

1. The accuser of the crime must be judged either guilty or not guilty.  

2. Under the U.S. system of justice, the individual on trial is initially presumed not guilty.  

3. Only STRONG EVIDENCE to the contrary causes the not guilty claim to be rejected in favor of a guilty verdict. 

4. The phrase "beyond a reasonable doubt" is often used to set the cutoff value for when enough evidence has been given to convict.

Theoretically, we should never say "The person is innocent." but instead "There is not sufficient evidence to show that the person is guilty."

Now let's compare that to how we look at a hypothesis test.

1. The decision about the population parameter(s) must be judged to follow one of two hypotheses.
	
2. We initially assume that $H_0$ is true.
	
3. The null hypothesis $H_0$ will be rejected (in favor of $H_a$) only if the sample evidence strongly suggests that $H_0$ is false.  If the sample does not provide such evidence, $H_0$ will not be rejected.

4.  The analogy to "beyond a reasonable doubt" in hypothesis testing is what is known as the **significance level**.  This will be set before conducting the hypothesis test and is denoted as $\alpha$.  Common values for $\alpha$ are 0.1, 0.01, and 0.05.

### Two possible conclusions

Therefore, we have two possible conclusions with hypothesis testing:

 - Reject $H_0$                
 - Fail to reject $H_0$
	
Gut instinct says that "Fail to reject $H_0$" should say "Accept $H_0$" but this technically is not correct.  Accepting $H_0$ is the same as saying that a person is innocent.  We cannot show that a person is innocent; we can only say that there was not enough substantial evidence to find the person guilty.

When you run a hypothesis test, you are the jury of the trial.  You decide whether there is enough evidence to convince yourself that $H_a$ is true ("the person is guilty") or that there was not enough evidence to convince yourself $H_a$ is true ("the person is not guilty").  You must convince yourself (using statistical arguments) which hypothesis is the correct one given the sample information.

**Important note:** Therefore, DO NOT WRITE "Accept $H_0$" any time you conduct a hypothesis test.  Instead write "Fail to reject $H_0$".

### Basic Logic of Hypothesis Testing

- Take a random sample (or samples) from a population (or two populations)
- If the sample data are consistent with the null hypothesis, do not reject the null hypothesis.
- If the sample data are inconsistent with the null hypothesis (in the direction of the alternative hypothesis), reject the null hypothesis and conclude that the alternative hypothesis is true (based on the particular sample collected).

### Statistical Significance

The idea that sample results are more extreme than we would reasonably expect to see by random chance if the null hypothesis were true is the fundamental idea behind statistical hypothesis tests.  If data as extreme would be very unlikely if the null hypothesis were true, we say the data are **statistically significant**.  Statistically significant data provide convincing evidence against the null hypothesis in favor of the alternative, and allow us to generalize our sample results to the claim about the population.

*** 
**Definition: Statistical Significance**

When results as extreme as the observed sample statistic are unlikely to occur by random chance alone (assuming the null hypothesis is true), we say the sample results are *statistically significant*.  If our sample is statistically significant, we have convincing evidence against $H_0$ and in favor of $H_a$.
***

## Randomization

We will now focus on building hypotheses looking at the difference between two population means in an example.  We will denote population means using the Greek symbol $\mu$ (pronounced "mu").  Thus, we will be looking to see if one group "out-performs" another group.  This is the quite possibly the most common type of statistical inference and serves as a basis for many other types of analyses when comparing two groups. 

Our null hypothesis will be of the form $H_0: \mu_1 = \mu_2$, which can also be written as $H_0: \mu_1 - \mu_2 = 0$.  Our alternative hypothesis will be of the form $H_0: \mu_1 \star \mu_2$ (or $H_a:  \mu_1 - \mu_2 \, \star \, 0$) where $\star$ = $<$, $\ne$, or $>$  depending on the context of the problem.  You needn't focus on these new symbols too much at this point.  It will just be a shortcut way for us to describe our hypotheses.

As we saw in Chapter \@ref(infer-basics), simulation and bootstrapping are valuable tools when conducting inferences based on one population variable.  We will see that the process of **randomization**, which is a resampling procedure similar to bootstrapping in some ways will be valuable in conducting tests comparing quantitative values from two groups.

***
```{block lc7-1, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  What is wrong about saying "The defendant is innocent." based on the US system of criminal trials?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  What is the purpose of hypothesis testing?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  What are some flaws with hypothesis testing?  How could we alleviate them?

***

### Comparing Action and Romance Movies

The `movies` data set in the `ggplot2movies` package contains information on a large number of movies that have been rated by users of IMDB.com.  We are interested in the question here of whether `Action` movies are rated higher on IMDB than `Romance` movies.  We will first need to do a little bit of data manipulation using the ideas from Chapter \@ref(manip) to get the data in the form that we would like:

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2movies)
movies_trimmed <- movies %>% 
  select(title, year, rating, Action, Romance)
movies_trimmed
```

Note that `Action` and `Romance` are binary variables here.  To remove any overlap of movies (and potential confusion) that are both `Action` and `Romance`, we will remove them from our _population_:

```{r}
movies_trimmed <- movies_trimmed %>%
  filter(!(Action == 1 & Romance == 1))
```

We will now create a new variable called `genre` that specifies whether a movie in our `movies_trimmed` data frame is an `"Action"` movie, a `"Romance"` movie, or `"Neither"`.  We aren't really interested in the `"Neither"` category here so we will exclude those rows as well.  Lastly, the `Action` and `Romance` columns are not needed anymore since they are encoded in the `genre` column.

```{r}
movies_trimmed <- movies_trimmed %>%
  mutate(genre = ifelse(Action == 1, "Action",
                        ifelse(Romance == 1, "Romance",
                               "Neither"))) %>%
  filter(genre != "Neither") %>%
  select(-Action, -Romance)
```
  
We are left with `r nrow(movies_trimmed)` movies in our _population_ data set that focuses on only `"Action"` and `"Romance"` movies.  
    
***
```{block lc7-2, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are the different genre variables stored as binary variables stored as 1s and 0s instead of just listing the genre as a column of values like "Action", "Comedy", etc.?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What complications could come above with us excluding action romance movies?  Should we question the results of our hypothesis test?  Explain.

***    

Let's now visualize the distributions of `rating` across both levels of `genre`.  Think about what type(s) of plot is/are appropriate here before you proceed:

```{r fig.cap="Rating vs genre in the population"}
library(ggplot2)
movies_trimmed %>% ggplot(aes(x = genre, y = rating)) +
  geom_boxplot()
```

We can see that the middle 50% of ratings for `"Action"` movies is more spread out than that of `"Romance"` movies in the population.  `Romance` has outliers at both the top and bottoms of the scale though.  We are initially interested in comparing the mean `rating` across these two groups so a faceted histogram may also be useful:

```{r warning=FALSE, fig.cap="Faceted histogram of genre vs rating"}
movies_trimmed %>% ggplot(mapping = aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white", fill = "dodgerblue") +
  facet_wrap(~genre)
```

**Important note:** Remember that we hardly ever have access to the population values as we do here.  This example and the `nycflights13` data set were used to create a common flow from chapter to chapter.  In nearly all circumstances, we'll be needing to use only a sample of the population to try to infer conclusions about the unknown population parameter values.  These examples do show a nice relationship between statistics (where data is usually small and more focused on experimental settings) and data science (where data is frequently large and collected without experimental conditions).  We'll learn more about observational studies and experiments in Chapter \@ref(regress).

### Sampling -> Randomization
    
We can use hypothesis testing to investigate ways to determine, for example, whether a **treatment** has an effect over a **control** and other ways to statistically analyze if one group performs better than, worse than, or different than another.  We will also use confidence intervals to determine the size of the effect if it exists. You'll see more on this in Chapter \@ref(ci).

We are interested here in seeing how we can use a random sample of action movies and a random sample of romance movies from `movies` to determine if a statistical difference exists in the mean ratings of each group.

***
```{block lc7-3a, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Define the relevant parameters here in terms of the populations of movies.

***  

Let's select a random sample of 34 action movies and a random sample of 34 romance movies.  (The number 34 was chosen somewhat arbitrarily here.)

```{r}
library(dplyr)
set.seed(2016)
movies_genre_sample <- movies_trimmed %>% 
  group_by(genre) %>%
  sample_n(34)
```

We can now observe the distributions of our two sample ratings for both groups.  Remember that these plots should
be rough approximations of our population distributions.

```{r fig.cap="Genre vs rating for our sample"}
movies_genre_sample %>% ggplot(aes(x = genre, y = rating)) +
  geom_boxplot()
```

```{r warning=FALSE, fig.cap="Genre vs rating for our sample as faceted histogram"}
movies_genre_sample %>% ggplot(mapping = aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white", fill = "dodgerblue") +
  facet_wrap(~genre)
```

***
```{block lc7-3b1, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What single value could we change to improve the approximation using the sample distribution on the population distribution?

***

Do we have reason to believe, based on the sample distributions of `rating` over the two groups of `genre`, that there is a significant difference between the mean `rating` for action movies compared to romance movies?  It's hard to say just based on the plots.  The boxplot does show that the median sample rating is higher romance movies, but the histogram isn't as clear.  The two groups have somewhat differently shaped distributions but they are both over similar values of `rating`.  It's often useful to calculate the mean and standard deviation as well conditioned on the two levels.

```{r}
summary_ratings <- movies_genre_sample %>% group_by(genre) %>%
  summarize(mean = mean(rating),
            std_dev = sd(rating))
summary_ratings
```

***
```{block lc7-3b2, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why did we not specify `na.rm = TRUE` here as we did in Chapter \@ref(manip)?

***

We see that the sample mean rating for romance movies, $\bar{x}_{r}$, is greater than the similar measure for action movies, $\bar{x}_a$.  But is it statistically significantly greater (thus, leading us to conclude that the means are statistically different)?  The standard deviation can provide some insight here but with these standard deviations being so similar it's still hard to say for sure.

***
```{block lc7-3b3, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why might the standard deviation provide some insight about the means being statistically different or not?

***

The hypotheses we specified can also be written in another form to better give us an idea of what we will be simulating to create our null distribution.

- $H_0: \mu_r - \mu_a = 0$
- $H_a: \mu_r - \mu_a \ne 0$
  
We are, therefore, interested in seeing whether the difference in the sample means, $\bar{x}_r - \bar{x}_a$, is statistically different than 0. R has a built-in command that can calculate the difference in these two sample means.

```{r}
mean_ratings <- movies_genre_sample %>% group_by(genre) %>%
  summarize(mean = mean(rating))
obs_diff <- diff(mean_ratings$mean)
```

We see here that the `diff` function calculates $\bar{x}_r - \bar{x}_a = `r mean_ratings$mean[2]` - `r mean_ratings$mean[1]` = `r obs_diff`$.  We will now proceed similarly to how we conducted hypothesis tests in Chapter \@ref(infer-basics) using simulation.  We can look at this from a tactile point of view by using index cards.  There are $n_r = 34$ data elements corresponding to romance movies and $n_a = 34$ for action movies.  We can write the 34 ratings from our sample for romance movies on one set of 34 index cards and the 34 leniency scores for action movies on another set of 34 index cards.  (Note that the sample sizes need not be the same.)

The next step is to put the two stacks of index cards together, creating a new set of 68 cards.  If we assume that the two population means are equal, we are saying that there is no association between ratings and genre (romance vs action).  We can use the index cards to create two **new** stacks for romance and action movies.  First, we must shuffle all the cards thoroughly.  After doing so, in this case with equal values of sample sizes, we split the deck in half.

We then calculate the new sample mean rating of the romance deck, and also the new sample mean rating of the action deck.  This creates one simulation of the samples.  We next want to calculate a statistic from these two samples.  Instead of actually doing the calculation using index cards, we can use R as we have before to simulate this process.

***
```{block lc7-3b4, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** How would the tactile shuffling of index cards change if we had different samples of say 20 action movies and 60 romance movies?  Describe each step that would change.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are we taking the difference in the means of the cards in the new shuffled decks?

***

```{r message=FALSE, warning=FALSE}
library(mosaic)
shuffled_ratings <- movies_trimmed %>%
     mutate(rating = shuffle(rating)) %>% 
     group_by(genre) %>%
     summarize(mean = mean(rating))
diff(shuffled_ratings$mean)
```

The only new command here is `shuffle` from the `mosaic` package, which does what we would expect it to do.  It simulates a shuffling of the ratings between the two levels of `genre` just as we could have done with index cards.  We can now proceed in a similar way to what we have done previously in Chapter \@ref(infer-basics) by repeating this process many times to create a _null distribution_ of simulated differences in sample means.

```{r cache=TRUE}
many_shuffles <- do(10000) * 
  (movies_trimmed %>%
     mutate(rating = shuffle(rating)) %>% 
     group_by(genre) %>%
     summarize(mean = mean(rating))
   )
```

It is a good idea here to `View` the `many_shuffles` data frame via `View(many_shuffles)`.  We need to figure out a way to subtract the first value of `mean` from the second value of `mean` for each of the 10,000 simulations.  This is a little tricky but the `group_by` function comes to our rescue here:

```{r}
rand_distn <- many_shuffles %>%
  group_by(.index) %>%
  summarize(diffmean = diff(mean))
```

We can now plot the distribution of these simulated differences in means:

```{r fig.cap="Simulated differences in means histogram"}
rand_distn %>% ggplot(aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 20)
```

Remember that we are interested in seeing where our observed sample mean difference of `r diff(mean_ratings$mean)` falls on this null distribution.  We are interested in simply a difference here so "more extreme" corresponds to values in both tails on the distribution.  Let's shade our null distribution to show a visual representation of our $p$-value:

```{r fig.cap="Shaded histogram to show p-value"}
rand_distn %>% ggplot(aes(x = diffmean, fill = (abs(diffmean) >= obs_diff))) +
  geom_histogram(color = "white", bins = 20)
```

You may initially think there is an error here, but remember that the observed difference in means was `r obs_diff`.  It falls far outside the range of simulated differences.  We can add a vertical line to represent both it and its negative (since this is a two-tailed test) instead:

```{r fig.cap="Histogram with vertical lines corresponding to observed statistic"}
rand_distn %>% ggplot(aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 100) +
  geom_vline(xintercept = obs_diff, color = "red") +
  geom_vline(xintercept = -obs_diff, color = "red")
```

Based on this plot, we have evidence supporting the conclusion that the mean rating for romance movies is different from that of action movies.  (It doesn't really matter what significance level was chosen in this case.  Think about why.)  The next important idea is to better understand just how much higher of a mean rating can we expect the romance movies to have compared to that of action movies.  This can be addressed by creating a 95% confidence interval as we will explore in Chapter \@ref(ci).
  
***
```{block lc7-3b, type='learncheck'}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Conduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating?  Make sure to use the `%>%` as much as possible.  What was different and what was the same? 

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What conclusions can you make from viewing the faceted histogram looking at `rating` versus `genre` that you couldn't see when looking at the boxplot?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Describe in a paragraph how we used Allen Downey's diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What is the value of the $p$-value for the hypothesis test comparing the mean rating of romance to action movies?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Do the results of the hypothesis test match up with the original plots we made looking at the population of movies?  Why or why not?

***  

### Summary

To review, these are the steps one would take whenever you'd like to do a hypothesis test comparing
values from the distributions of two groups:

- Simulate many samples using a random process that matches the way
the original data were collected and that _assumes the null hypothesis is
true_. 

- Collect the values of a sample statistic for each sample to create
a _randomization distribution_.

- Assess the significance of the _original_ sample by determining where
its sample statistic lies in the randomization distribution.

- If the proportion of values as extreme or more extreme than the observed statistic in the randomization
distribution is smaller than the pre-determined significance level $\alpha$, we reject $H_0$.  Otherwise,
we fail to reject $H_0$.  (If no significance level is given, one can assume $\alpha = 0.05$.)

<!--
## Theory-based methods {#theory-hypo} (Put into lab instead)

## Non-rejection of null hypothesis (Put into lab instead)
-->
    
## What's to come?

This chapter examined how to conclude whether two sample statistics are statistically different.  This is the same thing as trying to conclude if the difference in sample statistics is statistically different from zero.  We will see that this value of zero plays an important role in confidence intervals as well.  We'll also see in Chapter \@ref(ci) the relationship between confidence intervals and two-sided hypothesis tests as we worked with in this chapter.